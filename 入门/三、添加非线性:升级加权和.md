# 添加非线性：升级加权和

**非线性(nonlinearity)**，这是一种输入到输出的映射关系。与之前简单 地计算输入元素的加权和不同，非线性会增强神经网络的表示能力，而且如果使用得当，它可以 提高各种预测任务的准确率。

## 1 非线性的定义及其优势

```typescript
// 双层神经网络
export function multiLayerPerceptronRegressionModelHidden() {
    const model = tf.sequential();
    model.add(tf.layers.dense({
        inputShape: [bostonData.numFeatures],
        units: 50,
        activation: 'sigmoid',
        kernelInitializer: 'leCunNormal'
    }))
    model.add(tf.layers.dense({units: 1}))
    model.summary()
    return model
}
```

这里模型在代码中对应的函数名称为**多层感知机(multilayer perceptron, MLP)**。用这个术语描述的神经网络通常具有两个特征:第一，它有一个简单的无环拓扑结构， 也就是说它属于**前馈神经网络(feedforward neural network)**；第二，它至少有一个隐藏层。

```bash
# model.summary()方法，它是一个生成模型报告的工具，可以在控制台中打印 TensorFlow.js 模型的拓扑结构
Layer (type)                 Output shape              Param #
=================================================================
dense_Dense1 (Dense) 		  [null,50] 				650 _________________________________________________________________ 
dense_Dense2 (Dense) 		  [null,1] 					51 ================================================================= 
Total params: 701
Trainable params: 701 
Non-trainable params: 0
# 第一列 各层的名称与类型
# 第二列 各层的输出形状
# 第三列 各层权重参数的总数 表示组成该层权重的所有参数的个数，对包含多个权重的层而言，则表示所有权重的参数的总和。 例如 形状为[12, 50]的核，另一个形状为[50]的偏差，那么该层的训练参数为12 × 50 + 50 = 650
# 模型的权重参数、可训练参数以及不可训练参数对应的总数
```



### 1.1 直观地理解神经网络中的非线性

隐藏层包含一个非线性激活函数(activation function)，即代码中定义的`activation: 'sigmoid'`。获得密集层的最终输出，可以通过对矩阵中的元素逐个调用sigmoid函数来实现。

> **激活函数**一词源自对生物神经元的研究。生物神经元通过**动作电位(action potential)**，也就是在细胞膜上激发电 压，实现互相之间的通信。生物神经元通常会通过名为**突触(synapse)**的接触点接收来自很多上游神经元的信号。 上游神经元会以不同的频率激发动作电位，从而释放**神经递质(neurotransmitter)**，并且打开(或关闭)突触离子 通道，这将进一步导致接收端神经元细胞膜上电压的变化。这其实和密集层中一个单元所进行的加权和非常相似， 12 只有当电压超过一定阈值时，接收端神经元才会真正产生动作电位，也就是被“激活”，然后进一步影响下游神 经元的状态。从这个角度来看，普通生物神经元的激活函数与 ReLU 函数非常类似。ReLU 函数在输 入未达到一定阈值前输出为 0(处于“死区”)，当超过阈值后会随着输入线性增长，直到达到某个饱和级别(图 中没有显示)。

<img src="media/image-20210707213706068.png" alt="image-20210707213706068" style="zoom:50%;" />

sigmoid 函数的图像是一条曲线，ReLU 函数的图像是两个线段的拼接。尽管这两个函数都是非线性的，但是它们都有一个属性，即函数上的每一点都是光滑且可微的 ，这样才可以通过它们进行反向传播算法 。深度学习中还有其他几种常用的可微非线性函数，包括 ReLU 函数和**双曲正切函数(hyperbolic tangent，简称 tanh)**。
$$
sigmoid：S(x)=\frac{1}{1 + e^{-x}}
$$

$$
relu(x) = {0: x < 0, x: x>=0}
$$

<img src="media/image-20210707213833926.png" alt="image-20210707213833926" style="zoom:50%;" />

#### 非线性与模型容量

机器学习模型的**容量(capacity)**通常是指，模型能够学习的输入和输出关系的范围。对于拥有隐藏层和非线性激活函数的神经网络，其容量要大于线性 回归模型的容量。解释了为什么双层神经网络能够比线性回归模型取得更好的测试集准确率。

```typescript
// 三层神经网络, 总参数量3251
export function multiLayerPerceptronRegressionModel2Hidden() {
    const model = tf.sequential();
    model.add(tf.layers.dense({
        inputShape: [bostonData.numFeatures],
        units: 50,
        activation: 'sigmoid', 
        kernelInitializer: 'leCunNormal'
    }))
    model.add(tf.layers.dense({
        units: 50,
        activation: 'sigmoid', 
        kernelInitializer: 'leCunNormal'
    }))
    model.add(tf.layers.dense({units: 1}))
    model.summary()
    return model
}
```



#### 避免只增加层而不增加非线性的谬误

如果不使用sigmoid激活函数，双层神经网络就和单层 线性回归模型没什么两样!

构建多层神经 网络时的一个常见注意事项:**必须在隐藏层中添加非线性激活函数**。如果不这么做，就会导致计 算资源与时间的浪费，以及潜在的计算上的不稳定性。

<img src="media/image-20210707220101469.png" alt="image-20210707220101469" style="zoom:50%;" />

> 移除 sigmoid 激活函数后，模型在训练集、验证集和测试 集上的最终损失值都升高了(退回到和之前的纯线性模型相当的水平)，其损 失曲线也变得更加不平滑。

#### 非线性与模型可解释性

非线性模型与线性模型获取权重值的 API 是完全相同的，只需在模型对象上或属于它的层对 象上调用 getWeights()。

```typescript
// 在模型训练后打印权重
model.layers[0].getWeights()[0].print();
```

选择深度模型而不是浅层模型，本质上是牺牲一些可解释性来 换取更大的模型容量。

### 1.2 超参数与超参数优化

像单元数量、核初始化器以及激活函数这样的参数都属于模型的超参数(hyperparameter)。 “超参数”标志着这些参数和模型的权重参数是截然不同的，权重参数是在训练时通过反向传播 实现自动更新，即通过 model.fit()调用。

超参数类型：

- 模型中密集层的数量
- 密集层的核的初始化器类型
- 使用权重正则化，所需设定的正则化因子
- 对层使用丢弃法(dropout)所需设定的丢弃率
- 训练使用的优化器，比如'sgd'与'adam'
- 训练模型的轮次数
- 优化器的学习率
- 随着训练的进行，逐渐减小学习率所需设定的减速的速率
- 训练的批次尺寸

选择合适的超参数的过程叫作**超参数优化(hyperparameter optimization)**或**超参数调优 (hyperparameter tuning)**，旨在找到一系列能使训练后的验证集损失降到最低的超参数。

超参数优化缺乏标准的、开箱即用的方法或者工具，基于这一现状，深度学习从业者一般遵循下面 3 个策略。

1. 首先，如果当前待解决的问题和已彻底解决的问题类似，那么就可以对当前问题应用类似的模型，并“继承”其中的超参数。之后，可以再基 于“继承”的起点，在较小的超参数空间中搜索合适的超参数。
2. 其次，经验丰富的从业者可以基于直觉对当前问题应该使用的超参数做出合理猜测。尽管在 绝大多数情况下，这样主观选择的超参数不是最优的，但是对后续微调来说是不错的起点。
3. 最后，对于只有一小部分超参数需要优化的情况(比如少于 4 个)，可以采用**网格搜索(grid search)**，也就是说遍历所有的超参数组合，针对每一种组合执行完整的训练流程并记录在验证集 上的损失，最后选用能带来最低验证损失的超参数组合。

```typescript
// 用简单的网格搜索寻找最优超参数的伪代码
function hyperparameterGridSearch() {
    for units of [10, 20, 50, 100, 200]: 
    	for learningRate of [1e-5, 1e-4, 1e-3, 1e-2]:
        	if validationLoss < minValidationLoss
                minValidationLoss := validationLoss
                bestUnits := units
                bestLearningRate := learningRate
    return [bestUnits, bestLearningRate]
}
```



## 2 输出端的非线性：分类任务的模型

分类任务在机器学习中也很常见，有些分类任务属于**二分类(binary classification)**，主要面向回答“是或否”这类问题。

### 2.1 二分类定义



### 2.2 度量二分类器的性能:准确率、精确率、召回率



### 2.3 ROC 曲线:展示二分类问题中的取舍关系



### 2.4 二元交叉熵:二分类问题的损失函数

## 3 多分类问题

### 3.1 对分类数据进行 one-hot 编码



### 3.2 归一化指数函数:softmax 函数



### 3.3 分类交叉熵:多分类问题的损失函数



### 3.4 混淆矩阵:更细粒度地分析多分类问题



## 4 EXAMPLE

